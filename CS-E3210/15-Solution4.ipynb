{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1- A point is correctly classified if the signs of the\n",
    "label, $y_i$, and of the prediction, $\\mathbf{w}^{T}\\mathbf{x}_i$, agree, thus\n",
    "$y_i\\mathbf{w}^{T}\\mathbf{x}_i >0$ for both the positive and the\n",
    "negative cases. Here we assume any $\\mathbf{x}$ not only the training examples.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2- An example is correctly classified if\n",
    "$y_i\\mathbf{w}^{T}\\mathbf{x}_i >0$. We have two cases: \n",
    "\n",
    "$y_i\\mathbf{w}^{T}\\mathbf{x}_i \\ge 1$, then $\\xi_i=0$,\n",
    "\n",
    "$y_i\\mathbf{w}^{T}\\mathbf{x}_i <1$ \n",
    "and\n",
    "$y_i\\mathbf{w}^{T}\\mathbf{x}_i >0$, then\n",
    "$1>\\xi_i=1-y_i\\mathbf{w}^{T}\\mathbf{x}_i >0$, \n",
    "\n",
    "Finally we have $0\\le \\xi_i <1 $. Be aware, in the second case the\n",
    "strict inequality $\\xi_i<1$ is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3- Even if all examples are positive the SVM can process\n",
    "them. It can start from a feasible $\\mathbf{w}\\ne 0$ then for any\n",
    "$\\mathbf{x}_i$ of the training there is a $\\xi_i\\ge 0$ such that \n",
    "$\\mathbf{w}^T\\mathbf{x}_i\\ge 1-\\xi_i$ where the $y_i$ is dropped since\n",
    "it is equal to $1$. Then the SVM finds the largest optimal margin at\n",
    "the smallest loss and provides the optimal $\\mathbf{w}$ and $\\xi_i$s.\n",
    "In the optimum the majority of the examples have to be above the\n",
    "hyperplane  $\\mathbf{w}^T\\mathbf{x}_i=1$ otherwise the solution is not\n",
    "optimal. Now if the origin $\\mathbf{x}_0=\\mathbf{0}$ is substituted\n",
    "into the constraints we have $\\mathbf{w}^T\\mathbf{x}_0=0$, thus it is on\n",
    "the hyperplane defined $\\mathbf{w}^T\\mathbf{x}_0=0$, and below\n",
    "the hyperplane  $\\mathbf{w}^T\\mathbf{x}_i=1$. The distance between the\n",
    "two hyperplanes is exactly the margin which is maximized by the SVM,\n",
    "therefore those positive examples are separated from the origin when\n",
    "the margin is maximized.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1]\n",
      "[0.2  0.19]\n",
      "[0.38  0.071]\n",
      "[0.342  0.1639]\n",
      "[0.5078  0.24751]\n",
      "[0.65702  0.122759]\n",
      "[0.65702  0.122759]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "## Load the data\n",
    "X=np.array([(0,1),(2,1),(2,-1),(0,-1),(-2,-1),(-2,1)])\n",
    "y=np.array([1,1,1,-1,-1,-1])\n",
    "\n",
    "m,n=X.shape\n",
    "\n",
    "## initialize w, eta, lambda\n",
    "w=np.zeros(n)\n",
    "eta=0.1\n",
    "xlambda=1.0\n",
    "for i in range(m):            ## process all sample examples\n",
    "  if y[i]*np.dot(w,X[i])<1:   ## check the conditions\n",
    "    J=-y[i]*X[i]+xlambda*w    ## compute gradient\n",
    "  else:\n",
    "    J=xlambda*w               ## compute gradient\n",
    "  w=w-eta*J                   ## update w\n",
    "\n",
    "  print(w)\n",
    "\n",
    "print(w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
